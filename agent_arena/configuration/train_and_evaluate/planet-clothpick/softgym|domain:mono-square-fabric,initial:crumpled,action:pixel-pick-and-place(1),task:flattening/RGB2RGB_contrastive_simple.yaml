device: 'cuda:0'
train_mode: 'offline'

output_obs: "input_obs"
input_obs: "rgb"

deterministic_latent_dim: 200
stochastic_latent_dim: 30
action_dim: 4
hidden_dim: 200
embedding_dim: 1024
min_std_dev: 0.1
activation: 'relu'
trans_layers: 1
input_obs_dim: [3, 64, 64]
output_obs_dim: [3, 64, 64]
encoder_batchnorm: False
encoder_residual: False
decoder_batchnorm: False
decoder_residual: False
no_op: [1, 1, 1, 1]
free_nats: 1
symlog: False

observation_scale: 1.0
reward_scale: 1.0
reward_gradient_stop: False
kl_balance: 0.8
kl_scale: 1.0
kl_balancing: True

overshooting_distance: 2  
kl_overshooting_scale: 0.1
kl_overshooting_warmup: True
kl_overshooting_balance: 1.0
reward_overshooting_scale: 0.1
reward_overshooting_warmup: False

cost_fn: "trajectory_return" 

optimiser_class: 'adam'
optimiser_params: 
  lr: 1e-3
  eps: 1e-4
grad_clip_norm: 1000

batch_size: 50
sequence_size: 50
update_steps: 50001
test_interval: 1000

transform:
  name: 'pick_and_place_transformer'
  params:
    img_dim: [64, 64]
    remap_image: [-0.5, 0.5]
    device: "cuda:0"
    bit_depth: 5
    rgb_noise_var: 1.0
    random_rotation: True
    random_terminal: False
    rotation_degree: 90
    vertical_flip: True
    random_resize: False
    action_noise: 0.0

encoder_mode: "contrastive"
contrastive_dim: 128
contrastive_scale: 1.0
encoder_tau: 0.005
update_contrastive_target_interval: 2
anchor_transformer:
  name: "contrastive_learning_transformer"
  params:
      device: "cuda:0"
      img_dim: [64, 64]
      rgb_norm_mode: ['remap']
      rgb_norm_param:
        remap_range: [-0.5, 0.5]
      rgb_noise_var: 0.1
      rgb_blur: True
      rgb_blur_kernel_size: 3
      rgb_blur_sigma: 1.0
      rgb_color_jitter: True
      rgb_color_jitter_brightness: 0.5
      rgb_color_jitter_contrast: 0
      rgb_color_jitter_saturation: 0
      rgb_color_jitter_hue: 0.3


positive_transformer:
  name: "contrastive_learning_transformer"
  params:
      device: "cuda:0"
      img_dim: [64, 64]
      rgb_norm_mode: ['remap']
      rgb_norm_param:
        remap_range: [-0.5, 0.5]
      rgb_noise_var: 0.1
      rgb_blur: True
      rgb_blur_kernel_size: 3
      rgb_blur_sigma: 1.0
      rgb_color_jitter: True
      rgb_color_jitter_brightness: 0.5
      rgb_color_jitter_contrast: 0
      rgb_color_jitter_saturation: 0
      rgb_color_jitter_hue: 0.3
  

logger_name: "pick_and_place_fabric_single_task_logger"


datasets:
  - key: 'train'

    name: 'mono-square-fabric-pick-and-place'
    params:
      random_seed: 0
      num_episodes: 56000
      raw_img_dim: [128, 128]
      cross_traj: True
      return_pick_and_place_action_z: False
      return_rgb: True
      return_depth: False
      return_mask: False
      sequence_len: 50
      action_horizon: 20
      episode_len: 20
      mode: 'train'
      reward_mode: "hoque_ddpg"
      flatten_bonus: 0.5
      penalise_action_threshold: 0.7
      extreme_action_penalty: -0.5
      misgrasping_penalty: -0.5
      unflatten_penalty: -0.5
      unflatten_threshold: 0.98
      misgrasping_threshold: 1.0
      flattening_threshold: 0.98


  - key: 'test'

    name: 'mono-square-fabric-pick-and-place'
    params:
      random_seed: 0
      num_episodes: 100
      raw_img_dim: [128, 128]
      cross_traj: True
      return_pick_and_place_action_z: False
      return_rgb: True
      return_depth: False
      return_mask: False
      sequence_len: 50
      action_horizon: 20
      episode_len: 20
      mode: 'test'
      flatten_bonus: 0.5
      penalise_action_threshold: 0.7
      extreme_action_penalty: -0.5
      misgrasping_penalty: -0.5
      unflatten_penalty: -0.5
      unflatten_threshold: 0.98
      misgrasping_threshold: 1.0
      flattening_threshold: 0.98

    

policy:
  name: 'rect_fabric_cloth_mask_mpc'
  params:
    action_lower_bound: -1
    action_upper_bound: 1

    candidates: 5000
    planning_horizon: 1
    iterations: 100
    clip: True
    cloth_mask: "from_env"


    flatten_threshold: 0.99
    no_op: [[1.0, 1.0, 1.0, 1.0]]

    cost_fn: 'from_model'


test_horizons: [1, 2, 4] # This shoulud be smaller than action horizon.
action_horizon: 50
eval_episodes: 100
eval_save_latent: True
eval_save_obs_embedding: True